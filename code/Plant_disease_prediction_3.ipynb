{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zYppuP2pCW1v"
      },
      "outputs": [],
      "source": [
        "# %pip install opencv-python-headless numpy scikit-learn tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CormFB8rCUlW",
        "outputId": "2d615e2f-a904-435e-dd3c-ff862a375c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OoJFsxL3CqJy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        ")  # For proper train/validation split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import (\n",
        "    ImageDataGenerator,\n",
        ")  # For data augmentation\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdYeZpQJCrzN",
        "outputId": "f3650d28-2d6a-4b69-ee63-c63dd2e13669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 6\n",
            "Class names: ['healthy_leaf_1', 'early_leaf_spot_1', 'rust_1', 'early_rust_1', 'nutrition_deficiency_1', 'late_leaf_spot_1']\n"
          ]
        }
      ],
      "source": [
        "# 1. Dataset Path and Organization:\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/Projects/Plant Disease Prediction/Groundnut_Leaf_dataset\"  # Your dataset path\n",
        "train_dir = os.path.join(dataset_path, \"train\")\n",
        "test_dir = os.path.join(dataset_path, \"test\")\n",
        "\n",
        "# Get the class names (disease names)\n",
        "class_names = os.listdir(train_dir)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class names: {class_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WTfCDfwGCw6x"
      },
      "outputs": [],
      "source": [
        "# 2. Data Loading and Preprocessing:\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for split_dir in [train_dir, test_dir]:  # Loop through train and test\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(split_dir, class_name)\n",
        "        for filename in os.listdir(class_dir):\n",
        "            if filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "                img_path = os.path.join(class_dir, filename)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, (224, 224))  # Resize\n",
        "                    images.append(img)\n",
        "                    labels.append(class_name)\n",
        "                else:\n",
        "                    print(f\"Error reading image: {img_path}\")\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Kb_w27lfDKRR"
      },
      "outputs": [],
      "source": [
        "# Label Encoding:\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9V-rzwgDIgl",
        "outputId": "2b7c977e-5499-49b7-e4b6-a408336aaf16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape: (24, 224, 224, 3)\n",
            "Validation data shape: (6, 224, 224, 3)\n",
            "Test data shape: (10351, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "# # Split the training data into train and validation sets:\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     images[:len(os.listdir(train_dir)*5)], labels_encoded[:len(os.listdir(train_dir)*5)], test_size=0.2, random_state=42, stratify=labels_encoded[:len(os.listdir(train_dir)*5)] # 80% train, 20% validation\n",
        "# )\n",
        "\n",
        "# X_test = images[len(os.listdir(train_dir)*5):]\n",
        "# y_test = labels_encoded[len(os.listdir(train_dir)*5):]\n",
        "\n",
        "# print(\"Train data shape:\", X_train.shape)\n",
        "# print(\"Validation data shape:\", X_val.shape)\n",
        "# print(\"Test data shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7pfnWHwoFsF",
        "outputId": "d4c3dec1-decc-4002-d157-51d0fe8a7400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape: (8304, 224, 224, 3)\n",
            "Validation data shape: (1038, 224, 224, 3)\n",
            "Test data shape: (1039, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming `images` is a numpy array of shape (number_of_images, 224, 224, 3)\n",
        "# and `labels_encoded` is the corresponding labels\n",
        "\n",
        "# Calculate the number of samples to use for training and testing\n",
        "num_samples = len(images)\n",
        "train_size = int(0.8 * num_samples)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    images, labels_encoded, test_size=0.2, random_state=42, stratify=labels_encoded\n",
        ")\n",
        "\n",
        "# Further split the test set into validation and test sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
        ")\n",
        "\n",
        "print(\"Train data shape:\", X_train.shape)\n",
        "print(\"Validation data shape:\", X_val.shape)\n",
        "print(\"Test data shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MLKZ3pbDMfF"
      },
      "outputs": [],
      "source": [
        "# 3. Data Augmentation (Important for small datasets):\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\",\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)  # Fit the datagen on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_hQEGwEDNol"
      },
      "outputs": [],
      "source": [
        "# 4. CNN Model Building:\n",
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Conv2D(\n",
        "            32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)\n",
        "        ),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            64, (3, 3), activation=\"relu\"\n",
        "        ),  # Added another Conv layer\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),  # Increased Dense layer units\n",
        "        tf.keras.layers.Dropout(0.5),  # Added dropout for regularization\n",
        "        tf.keras.layers.Dense(\n",
        "            num_classes, activation=\"softmax\"\n",
        "        ),  # Output layer with softmax\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPgr6YKLDry7"
      },
      "outputs": [],
      "source": [
        "# 5. Model Training with Data Augmentation:\n",
        "epochs = 20  # Adjust as needed\n",
        "batch_size = 32  # Adjust as needed\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=batch_size),  # Use datagen.flow\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),  # Use validation data\n",
        "    steps_per_epoch=len(X_train) // batch_size,  # Calculate steps per epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR-xAL73EMVO"
      },
      "outputs": [],
      "source": [
        "# 6. Model Evaluation:\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKQP6Dt1CHVa"
      },
      "outputs": [],
      "source": [
        "# 7. Saving the Model and Label Encoder:\n",
        "\n",
        "models_dir = \"/content/drive/MyDrive/saved_models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "model_save_path = os.path.join(models_dir, \"groundnut_disease_model\")\n",
        "model.save(model_save_path)\n",
        "\n",
        "le_save_path = os.path.join(models_dir, \"label_encoder.pkl\")\n",
        "\n",
        "with open(le_save_path, \"wb\") as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "print(f\"Label encoder saved to: {le_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxnH5cj3EsVE"
      },
      "outputs": [],
      "source": [
        "# 8. Plotting Training History\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
