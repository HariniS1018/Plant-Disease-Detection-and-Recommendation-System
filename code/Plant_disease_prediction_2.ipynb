{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %pip install opencv-python-headless numpy scikit-learn tensorflow"
      ],
      "metadata": {
        "id": "zYppuP2pCW1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CormFB8rCUlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split  # For proper train/validation split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For data augmentation\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OoJFsxL3CqJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Dataset Path and Organization:\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/Projects/Plant Disease Prediction/Groundnut_Leaf_dataset'  # Your dataset path\n",
        "train_dir = os.path.join(dataset_path, 'train')\n",
        "test_dir = os.path.join(dataset_path, 'test')\n",
        "\n",
        "# Get the class names (disease names)\n",
        "class_names = os.listdir(train_dir)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class names: {class_names}\")\n"
      ],
      "metadata": {
        "id": "cdYeZpQJCrzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data Loading and Preprocessing:\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for split_dir in [train_dir, test_dir]:  # Loop through train and test\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(split_dir, class_name)\n",
        "        for filename in os.listdir(class_dir):\n",
        "            if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                img_path = os.path.join(class_dir, filename)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, (224, 224))  # Resize\n",
        "                    images.append(img)\n",
        "                    labels.append(class_name)\n",
        "                else:\n",
        "                    print(f\"Error reading image: {img_path}\")\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "WTfCDfwGCw6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding:\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)"
      ],
      "metadata": {
        "id": "Kb_w27lfDKRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training data into train and validation sets:\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    images[:len(os.listdir(train_dir)*5)], labels_encoded[:len(os.listdir(train_dir)*5)], test_size=0.2, random_state=42, stratify=labels_encoded[:len(os.listdir(train_dir)*5)] # 80% train, 20% validation\n",
        ")\n",
        "\n",
        "X_test = images[len(os.listdir(train_dir)*5):]\n",
        "y_test = labels_encoded[len(os.listdir(train_dir)*5):]\n",
        "\n",
        "print(\"Train data shape:\", X_train.shape)\n",
        "print(\"Validation data shape:\", X_val.shape)\n",
        "print(\"Test data shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "d9V-rzwgDIgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Data Augmentation (Important for small datasets):\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)  # Fit the datagen on the training data"
      ],
      "metadata": {
        "id": "6MLKZ3pbDMfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. CNN Model Building:\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), # Added another Conv layer\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'), # Increased Dense layer units\n",
        "    tf.keras.layers.Dropout(0.5),  # Added dropout for regularization\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax') # Output layer with softmax\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "B_hQEGwEDNol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Model Training with Data Augmentation:\n",
        "epochs = 20  # Adjust as needed\n",
        "batch_size = 32 # Adjust as needed\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=batch_size),  # Use datagen.flow\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),  # Use validation data\n",
        "    steps_per_epoch=len(X_train) // batch_size  # Calculate steps per epoch\n",
        ")"
      ],
      "metadata": {
        "id": "KPgr6YKLDry7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Model Evaluation:\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "FR-xAL73EMVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKQP6Dt1CHVa"
      },
      "outputs": [],
      "source": [
        "# 7. Saving the Model and Label Encoder:\n",
        "\n",
        "models_dir = '/content/drive/MyDrive/saved_models'\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "model_save_path = os.path.join(models_dir, 'groundnut_disease_model')\n",
        "model.save(model_save_path)\n",
        "\n",
        "le_save_path = os.path.join(models_dir, 'label_encoder.pkl')\n",
        "\n",
        "with open(le_save_path, 'wb') as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "print(f\"Label encoder saved to: {le_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Plotting Training History\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rxnH5cj3EsVE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}